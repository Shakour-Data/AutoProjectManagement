"""
Comprehensive unit tests for autoprojectmanagement/main_modules/task_workflow_management/do_urgent_tasks.py
Generated by AutoProjectManagement testing framework
"""

import pytest
import json
import os
import tempfile
from datetime import datetime
from unittest.mock import patch, MagicMock
from autoprojectmanagement.main_modules.task_workflow_management.do_urgent_tasks import (
    UrgentTaskExecutor, TaskStatus, TaskResult, setup_logging, 
    DEFAULT_TASK_BASE_TITLE, CRITICAL_TASK_COUNT, ADDITIONAL_TASK_COUNT
)

@pytest.fixture
def temp_dir():
    """Fixture for temporary directory."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield tmpdir

@pytest.fixture
def sample_task_result():
    """Fixture for sample task result."""
    return TaskResult(
        task_id="test_task_1",
        title="Test Urgent Task",
        status=TaskStatus.COMPLETED,
        start_time=datetime.now(),
        end_time=datetime.now(),
        duration_seconds=5.0,
        error_message=None,
        retry_count=0
    )

class TestUrgentTaskExecutorFunctionality:
    """Test class for UrgentTaskExecutor functionality tests (5 tests)"""
    
    def test_urgent_task_executor_initialization(self):
        """Test UrgentTaskExecutor initialization with default parameters."""
        executor = UrgentTaskExecutor()
        assert executor.logger is not None
        assert executor.results == []
    
    def test_urgent_task_executor_custom_logger(self):
        """Test UrgentTaskExecutor initialization with custom logger."""
        custom_logger = setup_logging()
        executor = UrgentTaskExecutor(logger=custom_logger)
        assert executor.logger == custom_logger
    
    def test_task_result_creation(self):
        """Test TaskResult creation with all parameters."""
        start_time = datetime.now()
        end_time = datetime.now()
        
        result = TaskResult(
            task_id="task_1",
            title="Test Task",
            status=TaskStatus.COMPLETED,
            start_time=start_time,
            end_time=end_time,
            duration_seconds=10.5,
            error_message=None,
            retry_count=0
        )
        
        assert result.task_id == "task_1"
        assert result.title == "Test Task"
        assert result.status == TaskStatus.COMPLETED
        assert result.start_time == start_time
        assert result.end_time == end_time
        assert result.duration_seconds == 10.5
        assert result.error_message is None
        assert result.retry_count == 0
    
    def test_task_result_is_successful(self):
        """Test TaskResult is_successful property."""
        # Test successful task
        result_success = TaskResult(
            task_id="task_1",
            title="Test Task",
            status=TaskStatus.COMPLETED,
            start_time=datetime.now()
        )
        assert result_success.is_successful is True
        
        # Test failed task
        result_failed = TaskResult(
            task_id="task_2",
            title="Test Task",
            status=TaskStatus.FAILED,
            start_time=datetime.now()
        )
        assert result_failed.is_successful is False
    
    def test_task_result_execution_time_str(self):
        """Test TaskResult execution_time_str property."""
        # Test with duration
        result_with_duration = TaskResult(
            task_id="task_1",
            title="Test Task",
            status=TaskStatus.COMPLETED,
            start_time=datetime.now(),
            duration_seconds=15.75
        )
        assert result_with_duration.execution_time_str == "15.75s"
        
        # Test without duration
        result_without_duration = TaskResult(
            task_id="task_2",
            title="Test Task",
            status=TaskStatus.COMPLETED,
            start_time=datetime.now()
        )
        assert result_without_duration.execution_time_str == "N/A"

class TestUrgentTaskExecutorEdgeCases:
    """Test class for UrgentTaskExecutor edge cases (5 tests)"""
    
    def test_generate_task_titles_count(self):
        """Test that generate_task_titles generates correct number of tasks."""
        executor = UrgentTaskExecutor()
        task_titles = executor._generate_task_titles()
        assert len(task_titles) == CRITICAL_TASK_COUNT + ADDITIONAL_TASK_COUNT
    
    def test_generate_task_titles_content(self):
        """Test that generate_task_titles generates tasks with correct base title."""
        executor = UrgentTaskExecutor()
        task_titles = executor._generate_task_titles()
        
        # Check that all tasks contain the base title
        for title in task_titles:
            assert DEFAULT_TASK_BASE_TITLE in title
    
    def test_execute_single_task_with_invalid_title(self):
        """Test execute_single_task with invalid task title."""
        executor = UrgentTaskExecutor()
        result = executor._execute_single_task("")
        
        # Should fail with invalid title
        assert result.status == TaskStatus.FAILED
        assert result.error_message is not None
    
    def test_execute_single_task_with_none_title(self):
        """Test execute_single_task with None task title."""
        executor = UrgentTaskExecutor()
        result = executor._execute_single_task(None)
        
        # Should fail with None title
        assert result.status == TaskStatus.FAILED
        assert result.error_message is not None
    
    def test_execute_urgent_tasks_empty_results(self):
        """Test execute_urgent_tasks with empty results."""
        executor = UrgentTaskExecutor()
        # If no tasks were executed, this should raise an error
        try:
            report = executor.get_summary_report()
            # If we get here, it means the test failed
            assert False, "Should have raised an error"
        except ValueError:
            # Expected behavior
            assert True

class TestUrgentTaskExecutorErrorHandling:
    """Test class for UrgentTaskExecutor error handling (5 tests)"""
    
    def test_execute_single_task_task_manager_exception(self):
        """Test execute_single_task handling of task manager exceptions."""
        with patch('autoprojectmanagement.main_modules.task_workflow_management.do_urgent_tasks.TaskManagement') as mock_task_manager:
            mock_task_manager.return_value.parse_creative_input.side_effect = Exception("Task manager error")
            
            executor = UrgentTaskExecutor()
            result = executor._execute_single_task("Test Task")
            
            # Should handle exception gracefully
            assert result.status == TaskStatus.FAILED
            assert "Task manager error" in result.error_message
    
    def test_execute_single_task_task_manager_no_id(self):
        """Test execute_single_task handling when task has no ID."""
        with patch('autoprojectmanagement.main_modules.task_workflow_management.do_urgent_tasks.TaskManagement') as mock_task_manager:
            mock_task = MagicMock()
            mock_task.id = None  # No ID
            mock_task_manager.return_value.parse_creative_input.return_value = mock_task
            
            executor = UrgentTaskExecutor()
            result = executor._execute_single_task("Test Task")
            
            # Should handle missing ID gracefully
            assert result.status == TaskStatus.FAILED
            assert result.error_message is not None
    
    def test_execute_single_task_task_manager_no_task(self):
        """Test execute_single_task handling when task manager returns None."""
        with patch('autoprojectmanagement.main_modules.task_workflow_management.do_urgent_tasks.TaskManagement') as mock_task_manager:
            mock_task_manager.return_value.parse_creative_input.return_value = None
            
            executor = UrgentTaskExecutor()
            result = executor._execute_single_task("Test Task")
            
            # Should handle None task gracefully
            assert result.status == TaskStatus.FAILED
            assert result.error_message is not None
    
    def test_execute_urgent_tasks_critical_exception(self):
        """Test execute_urgent_tasks handling of critical exceptions."""
        with patch.object(UrgentTaskExecutor, '_generate_task_titles') as mock_generate:
            mock_generate.side_effect = Exception("Critical error")
            
            executor = UrgentTaskExecutor()
            try:
                results = executor.execute_urgent_tasks()
                # If we get here, it means the test failed
                assert False, "Should have raised an error"
            except RuntimeError as e:
                # Expected behavior
                assert "Task execution failed" in str(e)
    
    def test_export_results_to_file_io_error(self):
        """Test export_results_to_file handling of IO errors."""
        executor = UrgentTaskExecutor()
        
        # Create a result to export
        result = TaskResult(
            task_id="task_1",
            title="Test Task",
            status=TaskStatus.COMPLETED,
            start_time=datetime.now()
        )
        executor.results = [result]
        
        # Try to export to an invalid path
        try:
            executor.export_results_to_file("/invalid/path/results.json")
            # If we get here, it means the test failed
            assert False, "Should have raised an error"
        except IOError:
            # Expected behavior
            assert True

class TestUrgentTaskExecutorIntegration:
    """Test class for UrgentTaskExecutor integration tests (5 tests)"""
    
    def test_urgent_task_executor_complete_workflow(self):
        """Test complete workflow of UrgentTaskExecutor."""
        t len(results) == CRITICAL_TASK_COUNT + ADDITIONAL_TASK_COUNT
    
    # Check that all results have valid data
    for result in results:
        assert result.task_id is not None
        assert result.title is not None
        assert result.status in [TaskStatus.COMPLETED, TaskStatus.FAILED]
        assert result.start_time is not None

def test_get_summary_report(self):
    """Test get_summary_report functionality."""
    executor = UrgentTaskExecutor()
    results = executor.execute_urgent_tasks()
    report = executor.get_summary_report()
        
        # Check report contains expected data
        assert "total_tasks" in report
        assert "completed_tasks" in report
        assert "failed_tasks" in report
        assert "success_rate" in report
        assert report["total_tasks"] == len(results)
    
    def test_export_results_to_file(self, temp_dir):
        """Test export_results_to_file functionality."""
        executor = UrgentTaskExecutor()
        results = executor.execute_urgent_tasks()
        
        export_path = os.path.join(temp_dir, "results.json")
        executor.export_results_to_file(export_path)
        
        # Check that file was created
        assert os.path.exists(export_path)
        
        # Check file content
        with open(export_path, 'r') as f:
            data = json.load(f)
        
        assert "summary" in data
        assert "detailed_results" in data
        assert len(data["detailed_results"]) == len(results)
    
    def test_setup_logging(self):
        """Test setup_logging functionality."""
        logger = setup_logging()
        assert logger is not None
        assert logger.level == 20  # INFO level
    
    def test_main_function(self):
        """Test main function execution."""
        # This is more of a smoke test since main() has side effects
        # We'll just check that it's callable
        try:
            from autoprojectmanagement.main_modules.task_workflow_management.do_urgent_tasks import main
            assert callable(main)
        except Exception:
            # If there's an import error, that's a problem
            assert False, "main function should be importable"

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
